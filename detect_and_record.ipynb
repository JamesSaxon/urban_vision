{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rOvvWAVTkMR7"
   },
   "source": [
    "# Object Detection Minimal Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard instructions are not reliable.  Follow these:\n",
    "\n",
    "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md\n",
    "\n",
    "These will install tensorflow and make it match with the models.\n",
    "\n",
    "Make sure your protoc is up to date.\n",
    "\n",
    "You may have to add the models to the path, explicitly, as I've done below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPs64QA1Zdov"
   },
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yn5_uV1HLvaz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import os \n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/media/jsaxon/brobdingnag/projects/urban_vision/models\")\n",
    "sys.path.append(\"/media/jsaxon/brobdingnag/projects/urban_vision/models/research\")\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import io\n",
    "# import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "import cv2\n",
    "\n",
    "cv2pil = lambda x : Image.fromarray(cv2.cvtColor(x, cv2.COLOR_BGR2RGB))\n",
    "np2pil = lambda x : Image.fromarray(x)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IogyryF2lFBL"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4YjnOjME1gy"
   },
   "outputs": [],
   "source": [
    "# @title Choose the model to use, then evaluate the cell.\n",
    "MODELS = {'centernet_with_keypoints': 'centernet_hourglass104_512x512_kpts_coco17_tpu-32', \n",
    "          'centernet_without_keypoints': 'centernet_hourglass104_512x512_coco17_tpu-8'}\n",
    "\n",
    "model_display_name = 'centernet_without_keypoints' # @param ['centernet_with_keypoints', 'centernet_without_keypoints']\n",
    "model_name = MODELS[model_display_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6917xnUSlp9x"
   },
   "source": [
    "### Build a detection model and load pre-trained model weights\n",
    "\n",
    "This sometimes takes a little while, please be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctPavqlyPuU_"
   },
   "outputs": [],
   "source": [
    "# Download the checkpoint and put it into models/research/object_detection/test_data/\n",
    "\n",
    "# !wget http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_hg104_512x512_coco17_tpu-8.tar.gz\n",
    "# !tar -xf centernet_hg104_512x512_coco17_tpu-8.tar.gz\n",
    "# !mv centernet_hg104_512x512_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4cni4SSocvP_"
   },
   "outputs": [],
   "source": [
    "pipeline_config = os.path.join('models/research/object_detection/configs/tf2/', model_name + '.config')\n",
    "model_dir       = 'models/research/object_detection/test_data/checkpoint/'\n",
    "\n",
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(model_config = model_config, is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(model_dir, 'ckpt-0')).expect_partial()\n",
    "\n",
    "def get_model_detection_function(model):\n",
    "    \"\"\"Get a tf.function for detection.\"\"\"\n",
    "  \n",
    "    @tf.function\n",
    "    def detect_fn(image):\n",
    "        \"\"\"Detect objects in image.\"\"\"\n",
    "    \n",
    "        image, shapes = model.preprocess(image)\n",
    "        prediction_dict = model.predict(image, shapes)\n",
    "        detections = model.postprocess(prediction_dict, shapes)\n",
    "    \n",
    "        return detections, prediction_dict, tf.reshape(shapes, [-1])\n",
    "  \n",
    "    return detect_fn\n",
    "\n",
    "detect_fn = get_model_detection_function(detection_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NKtD0IeclbL5"
   },
   "source": [
    "# Load label map data (for plotting).\n",
    "\n",
    "Label maps correspond index numbers to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a joke -- the examples are totally fucked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5mucYUS6exUJ"
   },
   "outputs": [],
   "source": [
    "label_map_path = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "label_map = label_map_util.load_labelmap(label_map_path)\n",
    "label_map = label_map_util.get_label_map_dict(label_map, use_display_name = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function to show detections through cv2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_OFFSET = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "color_cat = [(0, 0, 255), (0, 255, 255)]\n",
    "\n",
    "def paint_detections(img, detections, categories = [\"person\", \"bicycle\"], thresh = 0.3):\n",
    "    \n",
    "    thresh = 0.3\n",
    "\n",
    "    img = cv2.cvtColor(img.copy(), cv2.COLOR_RGB2BGR)\n",
    "    height, width, _ = img.shape\n",
    "\n",
    "    scores  = detections['detection_scores'] [0]\n",
    "    classes = detections['detection_classes'][0]\n",
    "    boxes   = detections['detection_boxes']  [0]\n",
    "\n",
    "    keep_categories = [label_map[k] - CATEGORY_OFFSET\n",
    "                       for k in categories]\n",
    "\n",
    "    display = np.isin(classes, keep_categories) & (scores > thresh)\n",
    "    \n",
    "    boxes   = boxes  [display]\n",
    "    scores  = scores [display]\n",
    "    classes = classes[display]\n",
    "\n",
    "    for box, cat in zip(boxes, classes):\n",
    "        \n",
    "        ymin, xmin, ymax, xmax = np.array(box)\n",
    "        \n",
    "        cv2.rectangle(img, tuple((int(xmin * width), int(ymin * height))),\n",
    "                           tuple((int(xmax * width), int(ymax * height))),\n",
    "                      color_cat[cat], 2)\n",
    "\n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):      return tf.train.Feature(int64_list = tf.train.Int64List(value = [value]))\n",
    "def _bytes_feature(value):      return tf.train.Feature(bytes_list = tf.train.BytesList(value = [value]))\n",
    "def _int64_list_feature(value): return tf.train.Feature(int64_list = tf.train.Int64List(value =  value ))\n",
    "def _bytes_list_feature(value): return tf.train.Feature(bytes_list = tf.train.BytesList(value =  value ))\n",
    "def _float_list_feature(value): return tf.train.Feature(float_list = tf.train.FloatList(value =  value ))\n",
    "\n",
    "\n",
    "def create_tfrecord(img, detections, thresh = 0.3, \n",
    "                    categories = [\"person\", \"bicycle\"], \n",
    "                    video_name = \"X\", frame_id = 0):\n",
    "    '''\n",
    "    Converts a dictionary of features for a single frame to a tf_example object.\n",
    "    '''\n",
    "    \n",
    "    ## Fixed values to pass in.\n",
    "    video         = str.encode(video_name)\n",
    "    source_id     = str.encode(str(\"{:05d}\".format(frame_id)))\n",
    "    \n",
    "    height        = img.shape[0]\n",
    "    width         = img.shape[1]\n",
    "\n",
    "    image_format  = str.encode('jpg')\n",
    "    \n",
    "    _, image_buff = cv2.imencode(\".jpg\", img.copy())\n",
    "    image_encoded = image_buff.tobytes()\n",
    "    \n",
    "    scores  = detections['detection_scores'] [0]\n",
    "    classes = detections['detection_classes'][0]\n",
    "    boxes   = detections['detection_boxes']  [0]\n",
    "\n",
    "    keep_categories = {label_map[k] - CATEGORY_OFFSET : k\n",
    "                       for k in categories}\n",
    "    \n",
    "    keep_detections = np.isin(classes, list(keep_categories)) & (scores > thresh)\n",
    "    \n",
    "    boxes   = boxes  [keep_detections]\n",
    "    scores  = scores [keep_detections]\n",
    "    classes = classes[keep_detections]\n",
    "                                   \n",
    "    np_box = np.array(boxes).astype(float)\n",
    "    xmins = list(np_box[:,1]) \n",
    "    xmaxs = list(np_box[:,3])\n",
    "    ymins = list(np_box[:,0]) \n",
    "    ymaxs = list(np_box[:,2]) \n",
    "\n",
    "    classes_text = []\n",
    "    for label in np.array(classes).astype(int):\n",
    "        classes_text.append(keep_categories[label].encode('utf-8'))\n",
    "    \n",
    "    classes = list(np.array(classes).astype(int) + CATEGORY_OFFSET)\n",
    "                               \n",
    "    record = tf.train.Example(features = tf.train.Features(feature={\n",
    "        'image/video'              : _bytes_feature(video),\n",
    "        'image/height'             : _int64_feature(height),\n",
    "        'image/width'              : _int64_feature(width),\n",
    "        'image/source_id'          : _bytes_feature(source_id),\n",
    "        'image/encoded'            : _bytes_feature(image_encoded),\n",
    "        'image/format'             : _bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin'   : _float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax'   : _float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin'   : _float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax'   : _float_list_feature(ymaxs),\n",
    "        'image/object/class/label' : _int64_list_feature(classes),\n",
    "        'image/object/class/text'  : _bytes_list_feature(classes_text),\n",
    "    }))\n",
    "                               \n",
    "    return record\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally, the detections, from stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vr_Fux-gfaG9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tfrecords_from_stream(video, ouput, N = 100, NSKIP = 10, VAL = 5, show = False):\n",
    "\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    ret, cv_img = cap.read()\n",
    "\n",
    "    training   = tf.io.TFRecordWriter(ouput + \"_train.tfrecord\")\n",
    "    validation = tf.io.TFRecordWriter(ouput + \"_val.tfrecord\")\n",
    "\n",
    "    for ix in tqdm.tqdm(range(N)):\n",
    "\n",
    "        for xi in range(NSKIP): ret, cv_img = cap.read()\n",
    "\n",
    "        pil_img = cv2pil(cv_img)\n",
    "        np_img  = np.array(pil_img)\n",
    "        tf_img  = tf.convert_to_tensor(np.expand_dims(np_img, 0), dtype=tf.float32)\n",
    "        tf_img8 = tf.cast(tf_img[0], tf.uint8)\n",
    "\n",
    "        detections, predictions_dict, shapes = detect_fn(tf_img)\n",
    "\n",
    "        record = create_tfrecord(cv_img, detections, thresh = 0.5, \n",
    "                                 video_name = video, frame_id = (ix+1) * NSKIP)\n",
    "        \n",
    "        if ix % VAL: training  .write(record.SerializeToString())\n",
    "        else:        validation.write(record.SerializeToString())\n",
    "\n",
    "        if show: \n",
    "            img_det = paint_detections(np_img, detections, thresh = 0.5)\n",
    "            display(cv2pil(img_det))\n",
    "\n",
    "        \n",
    "    training  .close()\n",
    "    validation.close()\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.43s/it]\n"
     ]
    }
   ],
   "source": [
    "video  = \"/media/jsaxon/brobdingnag/data/cv/vid/burnham/55/20200927_143601.MOV\"\n",
    "output = \"/media/jsaxon/brobdingnag/data/cv/tf/burnham/55/20200927_143601\"\n",
    "\n",
    "record = tfrecords_from_stream(video, output, N = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "features {\n",
       "  feature {\n",
       "    key: \"image/format\"\n",
       "    value {\n",
       "      bytes_list {\n",
       "        value: \"jpg\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"image/height\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 1520\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"image/object/bbox/xmax\"\n",
       "    value {\n",
       "      float_list {\n",
       "        value: 0.6568061709403992\n",
       "        value: 0.42628544569015503\n",
       "        value: 0.41199246048927307\n",
       "        value: 0.6452797651290894\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"image/object/bbox/xmin\"\n",
       "    value {\n",
       "      float_list {\n",
       "        value: 0.5986146330833435\n",
       "        value: 0.4073081612586975\n",
       "        value: 0.39413192868232727\n",
       "        value: 0.6103401184082031\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"image/object/bbox/ymax\"\n",
       "    value {\n",
       "      float_list {\n",
       "        value: 0.7727861404418945\n",
       "        value: 0.30351051688194275\n",
       "        value: 0.31569039821624756\n",
       "        value: 0.8514447808265686\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"image/object/bbox/ymin\"\n",
       "    value {\n",
       "      float_list {\n",
       "        value: 0.5021644830703735\n",
       "        value: 0.20435558259487152\n",
       "        value: 0.2211378663778305\n",
       "        value: 0.6515292525291443\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"image/object/class/label\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 2\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"image/object/class/text\"\n",
       "    value {\n",
       "      bytes_list {\n",
       "        value: \"person\"\n",
       "        value: \"person\"\n",
       "        value: \"person\"\n",
       "        value: \"bicycle\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"image/source_id\"\n",
       "    value {\n",
       "      bytes_list {\n",
       "        value: \"00010\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"image/video\"\n",
       "    value {\n",
       "      bytes_list {\n",
       "        value: \"/media/jsaxon/brobdingnag/data/cv/vid/burnham/55/20200927_143601.MOV\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"image/width\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 2720\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "inference_tf2_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
